{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0AIo+LxBCMLhfXGyQVneP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karinefonseca/NLP/blob/main/Exemplos_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVcIk-TPFOlJ",
        "outputId": "3ace246d-3049-4846-d28a-2e0d3fc04492"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y8C00Q0FqEJ",
        "outputId": "ed1b954d-af5a-400f-8241-544b25ad15ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenização\n",
        "\n",
        "###Descrição: Dividir o texto em unidades menores, como palavras ou frases.\n"
      ],
      "metadata": {
        "id": "3lUHY3_UFwcg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T6hJkvNFItj",
        "outputId": "cd0132fe-d113-48c2-8943-c42b052894d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Olá', ',', 'bem-vindo', 'ao', 'mundo', 'do', 'NLP', '!']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texto = \"Olá, bem-vindo ao mundo do NLP!\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remoção de Stop Words\n",
        "\n",
        "###Descrição: Remover palavras comuns que não adicionam muito significado ao texto, como \"e\", \"o\", \"a\"."
      ],
      "metadata": {
        "id": "6knzbOJcF8Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "filtered_tokens = [w for w in tokens if not w in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VcLGG-tGCB1",
        "outputId": "a231b29b-b0a4-46f1-b511-06d6438c5d01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Olá', ',', 'bem-vindo', 'mundo', 'NLP', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming e Lematização\n",
        "\n",
        "###Descrição: Reduzir as palavras às suas raízes (stemming) ou às suas formas base (lematização)"
      ],
      "metadata": {
        "id": "hZv1wC5vGVmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RSLPStemmer\n",
        "nltk.download('rslp')\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "stems = [stemmer.stem(w) for w in filtered_tokens]\n",
        "print(stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_1zZtj-GZvo",
        "outputId": "1a725531-872d-4f78-d63b-48ee1f018d3e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['olá', ',', 'bem-v', 'mund', 'nlp', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Análise de Sentimentos\n",
        "\n",
        "###Descrição: Identificar a emoção ou opinião expressa no texto."
      ],
      "metadata": {
        "id": "u7Txwh8uG25d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QnyqZ3SuG1l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "texto = \"I love NLP\"\n",
        "resultado = sentiment_pipeline(texto)\n",
        "print(f\"Sentimento: {resultado[0]['label']}, Score: {resultado[0]['score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT58Kn7yG166",
        "outputId": "8447a508-72e7-4061-aeda-b1b053fa6bb9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentimento: 5 stars, Score: 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Código Completo com Pipeline para Análise de Sentimentos"
      ],
      "metadata": {
        "id": "9tjEnwQiKwWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download dos recursos necessários\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "\n",
        "# Carregar o pipeline de análise de sentimento\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "class NLPPipeline:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "        self.tokens = None\n",
        "        self.filtered_tokens = None\n",
        "        self.stems = None\n",
        "        self.sentiment = None\n",
        "\n",
        "    def tokenize(self):\n",
        "        self.tokens = word_tokenize(self.text)\n",
        "        return self\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        self.filtered_tokens = [w for w in self.tokens if not w in stop_words]\n",
        "        return self\n",
        "\n",
        "    def stem(self):\n",
        "        stemmer = RSLPStemmer()\n",
        "        self.stems = [stemmer.stem(w) for w in self.filtered_tokens]\n",
        "        return self\n",
        "\n",
        "    def analyze_sentiment(self):\n",
        "        resultado = sentiment_pipeline(self.text)\n",
        "        self.sentiment = f\"Sentimento: {resultado[0]['label']}, Score: {resultado[0]['score']:.2f}\"\n",
        "        return self\n",
        "\n",
        "    def run(self):\n",
        "        return self.tokenize().remove_stopwords().stem().analyze_sentiment()\n",
        "\n",
        "# Exemplo de uso do pipeline\n",
        "texto = \"Hello, welcome to the world of NLP! Here, you will learn about natural language processing.\"\n",
        "\n",
        "pipeline = NLPPipeline(texto)\n",
        "result = pipeline.run()\n",
        "\n",
        "print(\"Tokens:\", pipeline.tokens)\n",
        "print(\"Tokens sem stop words:\", pipeline.filtered_tokens)\n",
        "print(\"Stems:\", pipeline.stems)\n",
        "print(\"Sentimento:\", pipeline.sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1-wAElkJU-4",
        "outputId": "2d15ae8b-a35d-4443-8b09-9cc06ba97922"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Hello', ',', 'welcome', 'to', 'the', 'world', 'of', 'NLP', '!', 'Here', ',', 'you', 'will', 'learn', 'about', 'natural', 'language', 'processing', '.']\n",
            "Tokens sem stop words: ['Hello', ',', 'welcome', 'world', 'NLP', '!', 'Here', ',', 'learn', 'natural', 'language', 'processing', '.']\n",
            "Stems: ['hell', ',', 'welcom', 'world', 'nlp', '!', 'her', ',', 'learn', 'natur', 'languag', 'processing', '.']\n",
            "Sentimento: Sentimento: 5 stars, Score: 0.80\n"
          ]
        }
      ]
    }
  ]
}